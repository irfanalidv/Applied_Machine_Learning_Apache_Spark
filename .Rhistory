setwd("C:/Users/Irfanalidv/Desktop/Applied_Machine_Learning_Apache_Spark")
# Databricks notebook source
library(SparkR)
#run Everytime
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "C:/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7")
}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
# Databricks notebook source
library(SparkR)
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
library(sparklyr)
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
library(sparklyr)
library(spatial)
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
Sys.setenv(SPARK_HOME='C:/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7',HADOOP_HOME='C:\\hadoop-common-2.2.0-bin-master (1)\\hadoop-common-2.2.0-bin-master\\bin')
.libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'),.libPaths()))
Sys.setenv('SPARKR_SUBMIT_ARGS'='"sparkr-shell"')
library(SparkR)
library(rJava)
sparkR.session(enableHiveSupport = FALSE,master = "local[*]", sparkConfig = list(spark.driver.memory = "1g",spark.sql.warehouse.dir="C:\\hadoop-common-2.2.0-bin-master (1)\\hadoop-common-2.2.0-bin-master\\bin"))
Sys.setenv(SPARK_HOME='C:/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7',HADOOP_HOME='C:\\hadoop-common-2.2.0-bin-master (1)\\hadoop-common-2.2.0-bin-master')
.libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'),.libPaths()))
library(SparkR)
library(rJava)
sparkR.session(enableHiveSupport = FALSE,master = "local[*]", sparkConfig = list(spark.driver.memory = "1g",spark.sql.warehouse.dir="C:\\hadoop-common-2.2.0-bin-master (1)\\hadoop-common-2.2.0-bin-master\\bin"))
df <- as.DataFrame(iris)
library(SparkR)
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)
localDF <- data.frame(name=c("John", "Smith", "Sarah"), age=c(19, 23, 18))
df <- createDataFrame(sqlContext, localDF)
sqlContext <- sparkRSQL.init(sc)
