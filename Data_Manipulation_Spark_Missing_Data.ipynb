{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"missingdata\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark.read.csv(\"/FileStore/tables/walmart/ContainsNull.csv\",header=True,inferSchema=True)\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Drop the missing data\n#df.na.drop(how='any', thresh=None, subset=None)\n##If 'any', drop a row if it contains any nulls.\n##If 'all', drop a row only if all its values are null.\n#param subset: \n#   optional list of column names to consider.\n# Drop any row that contains missing data\ndf.na.drop().show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Has to have at least 2 NON-null values\ndf.na.drop(thresh=2).show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df.na.drop(subset=[\"Sales\"]).show()\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.na.drop(how='any').show()\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df.na.drop(how='all').show()\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Fill the missing values\ndf.na.fill('NEW VALUE').show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df.na.fill(0).show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#based on column\ndf.na.fill('No Name',subset=['Name']).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df.na.fill(0,subset=\"Sales\").show()\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df.na.fill({'Sales':0,'Name':'No Name'}).show()\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#fill values with the mean value for the column\nfrom pyspark.sql.functions import mean\nmean_val = df.select(mean(df['Sales'])).collect()\n\n# Weird nested formatting of Row object!\nmean_val[0][0]\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["mean_sales = mean_val[0][0]\ndf.na.fill(mean_sales,[\"Sales\"]).show()\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#  one-liner\ndf.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"Data_Manipulation_Spark_Missing_Data","notebookId":3830219129576287},"nbformat":4,"nbformat_minor":0}
