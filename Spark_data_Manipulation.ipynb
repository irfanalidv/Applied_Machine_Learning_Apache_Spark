{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"walmart\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#loading data \ndf = spark.read.csv('/FileStore/tables/walmart/walmart_stock.csv',inferSchema=True,header=True)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.head(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df.describe().printSchema()\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import format_number\nresult = df.describe()\nresult.select(result['summary'],\n              format_number(result['Open'].cast('float'),2).alias('Open'),\n              format_number(result['High'].cast('float'),2).alias('High'),\n              format_number(result['Low'].cast('float'),2).alias('Low'),\n              format_number(result['Close'].cast('float'),2).alias('Close'),\n              result['Volume'].cast('int').alias('Volume')\n             ).show()\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df2 = df.withColumn(\"HV Ratio\",df[\"High\"]/df[\"Volume\"])#.show()\n# df2.show()\ndf2.select('HV Ratio').show()\ndf2.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df.orderBy(df[\"High\"].desc()).head(1)[0][0]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Also could have gotten this from describe()\nfrom pyspark.sql.functions import mean\ndf.select(mean(\"Close\")).show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import max,min\ndf.select(max(\"Volume\"),min(\"Volume\")).show()\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#df.filter(\"Close < 60\").count()\ndf.filter(df['Close'] < 60).count()\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#What percentage of the time was the High greater than 80 dollars ?\n#In other words, (Number of Days High>80)/(Total Days in the dataset)\n(df.filter(df[\"High\"]>80).count()*1.0/df.count())*100\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#What is the Pearson correlation between High and Volume?\nfrom pyspark.sql.functions import corr\ndf.select(corr(\"High\",\"Volume\")).show()\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#What is the max High per year?\nfrom pyspark.sql.functions import year\nyeardf = df.withColumn(\"Year\",year(df[\"Date\"]))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["max_df = yeardf.groupBy('Year').max()\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["max_df.select('Year','max(High)').show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#What is the average Close for each Calendar Month?\n#In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc...result will have a value for each of these months\nfrom pyspark.sql.functions import month\nmonthdf = df.withColumn(\"Month\",month(\"Date\"))\nmonthdf.show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["monthavgs = monthdf.select(\"Month\",\"Close\").groupBy(\"Month\").mean()\nmonthavgs.show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["monthavgs.select(\"Month\",\"avg(Close)\").orderBy('Month').show()\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Spark_data_Manipulation","notebookId":3830219129576256},"nbformat":4,"nbformat_minor":0}
